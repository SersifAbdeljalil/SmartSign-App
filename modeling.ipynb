{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4af48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Chemin vers le dataset complet\n",
    "DATASET_PATH = r\"C:\\Users\\sersi\\Desktop\\projet_SE_et_IOT\\ASL-Sensor-Dataglove-Dataset\\asl_dataset_complete.csv\"\n",
    "\n",
    "# Dossier pour sauvegarder le mod√®le\n",
    "MODEL_OUTPUT_DIR = r\"C:\\Users\\sersi\\Desktop\\projet_SE_et_IOT\\ASL-Sensor-Dataglove-Dataset\\model\"\n",
    "\n",
    "# Colonnes des features (capteurs)\n",
    "FEATURE_COLUMNS = [\n",
    "    'flex_1', 'flex_2', 'flex_3', 'flex_4', 'flex_5',  # 5 Flex Sensors\n",
    "    'GYRx', 'GYRy', 'GYRz',                            # 3 Gyroscope\n",
    "    'ACCx', 'ACCy', 'ACCz'                             # 3 Accelerometer\n",
    "]\n",
    "\n",
    "# Param√®tres du mod√®le\n",
    "TEST_SIZE = 0.2        # 20% pour le test\n",
    "RANDOM_STATE = 42      # Pour la reproductibilit√©\n",
    "N_ESTIMATORS = 100     # Nombre d'arbres dans la for√™t\n",
    "\n",
    "# ============================================\n",
    "# FONCTION 1: CHARGER ET EXPLORER LES DONN√âES\n",
    "# ============================================\n",
    "\n",
    "def load_and_explore_data():\n",
    "    \"\"\"\n",
    "    Charge le dataset et affiche des statistiques\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"üìä CHARGEMENT DES DONN√âES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Charger le dataset\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset charg√© avec succ√®s!\")\n",
    "    print(f\"   üìè Dimensions: {df.shape[0]} lignes √ó {df.shape[1]} colonnes\")\n",
    "    print(f\"\\nüìã Colonnes disponibles:\")\n",
    "    print(f\"   {list(df.columns)}\")\n",
    "    \n",
    "    # Informations sur les labels\n",
    "    print(f\"\\nüè∑Ô∏è  LABELS (Lettres/Mots):\")\n",
    "    print(f\"   Total de classes: {df['label'].nunique()}\")\n",
    "    print(f\"   Classes: {sorted(df['label'].unique())}\")\n",
    "    \n",
    "    # R√©partition des donn√©es\n",
    "    print(f\"\\nüìà R√âPARTITION PAR CLASSE:\")\n",
    "    label_counts = df['label'].value_counts().sort_index()\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"   {label:15s}: {count:6d} √©chantillons\")\n",
    "    \n",
    "    # Statistiques des features\n",
    "    print(f\"\\nüìä STATISTIQUES DES FEATURES:\")\n",
    "    print(df[FEATURE_COLUMNS].describe())\n",
    "    \n",
    "    # V√©rifier les valeurs manquantes\n",
    "    missing = df[FEATURE_COLUMNS].isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  VALEURS MANQUANTES:\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Aucune valeur manquante dans les features!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FONCTION 2: PR√âTRAITEMENT DES DONN√âES\n",
    "# ============================================\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Pr√©pare les donn√©es pour l'entra√Ænement\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîß PR√âTRAITEMENT DES DONN√âES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Extraire les features (X) et les labels (y)\n",
    "    X = df[FEATURE_COLUMNS].values\n",
    "    y = df['label'].values\n",
    "    \n",
    "    print(f\"\\nüìä Features (X): {X.shape}\")\n",
    "    print(f\"üè∑Ô∏è  Labels (y): {y.shape}\")\n",
    "    \n",
    "    # Encoder les labels (convertir lettres en nombres)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    print(f\"\\nüî¢ Encodage des labels:\")\n",
    "    for i, label in enumerate(label_encoder.classes_[:5]):  # Afficher les 5 premiers\n",
    "        print(f\"   '{label}' ‚Üí {i}\")\n",
    "    print(f\"   ... ({len(label_encoder.classes_)} classes au total)\")\n",
    "    \n",
    "    # Normaliser les features (StandardScaler)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Normalisation appliqu√©e (StandardScaler)\")\n",
    "    print(f\"   Moyenne avant: {X.mean(axis=0)[:3]} ...\")\n",
    "    print(f\"   Moyenne apr√®s: {X_scaled.mean(axis=0)[:3]} ...\")\n",
    "    \n",
    "    # S√©parer en train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_encoded, \n",
    "        test_size=TEST_SIZE, \n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_encoded  # Garde la m√™me proportion de classes\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä S√âPARATION TRAIN/TEST:\")\n",
    "    print(f\"   üéØ Train: {X_train.shape[0]} √©chantillons ({(1-TEST_SIZE)*100:.0f}%)\")\n",
    "    print(f\"   üß™ Test:  {X_test.shape[0]} √©chantillons ({TEST_SIZE*100:.0f}%)\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler, label_encoder\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FONCTION 3: ENTRA√éNER LE MOD√àLE\n",
    "# ============================================\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Entra√Æne un Random Forest Classifier\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ ENTRA√éNEMENT DU MOD√àLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Cr√©er le mod√®le\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=N_ESTIMATORS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,  # Utiliser tous les CPU\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüå≥ Mod√®le: Random Forest\")\n",
    "    print(f\"   Nombre d'arbres: {N_ESTIMATORS}\")\n",
    "    print(f\"   √âtat al√©atoire: {RANDOM_STATE}\")\n",
    "    \n",
    "    # Entra√Æner\n",
    "    print(f\"\\n‚è≥ Entra√Ænement en cours...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"‚úÖ Entra√Ænement termin√©!\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    print(f\"\\nüîÑ Validation crois√©e (5-fold)...\")\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    print(f\"   Scores: {cv_scores}\")\n",
    "    print(f\"   Moyenne: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FONCTION 4: √âVALUER LE MOD√àLE\n",
    "# ============================================\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, label_encoder):\n",
    "    \"\"\"\n",
    "    √âvalue les performances du mod√®le\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìà √âVALUATION DU MOD√àLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nüéØ ACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nüìä RAPPORT DE CLASSIFICATION:\")\n",
    "    print(\"-\"*60)\n",
    "    report = classification_report(\n",
    "        y_test, y_pred, \n",
    "        target_names=label_encoder.classes_,\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    print(f\"\\nüî≤ MATRICE DE CONFUSION:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Afficher seulement un aper√ßu si trop de classes\n",
    "    if len(label_encoder.classes_) <= 10:\n",
    "        print(cm)\n",
    "    else:\n",
    "        print(f\"   (Matrice {cm.shape[0]}√ó{cm.shape[1]} - voir visualisation)\")\n",
    "    \n",
    "    return accuracy, report, cm\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FONCTION 5: VISUALISER LES R√âSULTATS\n",
    "# ============================================\n",
    "\n",
    "def visualize_results(cm, label_encoder, feature_importance, model_dir):\n",
    "    \"\"\"\n",
    "    Cr√©e des visualisations des r√©sultats\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä CR√âATION DES VISUALISATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Matrice de confusion\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.title('Matrice de Confusion')\n",
    "    plt.ylabel('Vraie classe')\n",
    "    plt.xlabel('Classe pr√©dite')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    cm_path = os.path.join(model_dir, 'confusion_matrix.png')\n",
    "    plt.savefig(cm_path, dpi=150)\n",
    "    print(f\"   ‚úÖ Matrice de confusion: {cm_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Importance des features\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_names = FEATURE_COLUMNS\n",
    "    indices = np.argsort(feature_importance)[::-1]\n",
    "    \n",
    "    plt.bar(range(len(feature_importance)), feature_importance[indices])\n",
    "    plt.xticks(range(len(feature_importance)), \n",
    "               [feature_names[i] for i in indices], \n",
    "               rotation=45, ha='right')\n",
    "    plt.title('Importance des Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fi_path = os.path.join(model_dir, 'feature_importance.png')\n",
    "    plt.savefig(fi_path, dpi=150)\n",
    "    print(f\"   ‚úÖ Importance des features: {fi_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FONCTION 6: SAUVEGARDER LE MOD√àLE\n",
    "# ============================================\n",
    "\n",
    "def save_model(model, scaler, label_encoder, model_dir):\n",
    "    \"\"\"\n",
    "    Sauvegarde le mod√®le et les transformateurs\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üíæ SAUVEGARDE DU MOD√àLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Cr√©er le dossier\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Sauvegarder le mod√®le\n",
    "    model_path = os.path.join(model_dir, 'asl_model.pkl')\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"   ‚úÖ Mod√®le: {model_path}\")\n",
    "    \n",
    "    # Sauvegarder le scaler\n",
    "    scaler_path = os.path.join(model_dir, 'scaler.pkl')\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"   ‚úÖ Scaler: {scaler_path}\")\n",
    "    \n",
    "    # Sauvegarder le label encoder\n",
    "    encoder_path = os.path.join(model_dir, 'label_encoder.pkl')\n",
    "    with open(encoder_path, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(f\"   ‚úÖ Label Encoder: {encoder_path}\")\n",
    "    \n",
    "    # Sauvegarder les m√©tadonn√©es en JSON\n",
    "    metadata = {\n",
    "        'feature_columns': FEATURE_COLUMNS,\n",
    "        'classes': label_encoder.classes_.tolist(),\n",
    "        'n_classes': len(label_encoder.classes_),\n",
    "        'n_features': len(FEATURE_COLUMNS),\n",
    "        'model_type': 'RandomForestClassifier',\n",
    "        'n_estimators': N_ESTIMATORS\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(model_dir, 'model_metadata.json')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(f\"   ‚úÖ M√©tadonn√©es: {metadata_path}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FONCTION 7: TESTER UNE PR√âDICTION\n",
    "# ============================================\n",
    "\n",
    "def test_prediction(model, scaler, label_encoder, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Teste une pr√©diction sur un √©chantillon al√©atoire\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üß™ TEST DE PR√âDICTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # S√©lectionner un √©chantillon al√©atoire\n",
    "    idx = np.random.randint(0, len(X_test))\n",
    "    sample = X_test[idx:idx+1]\n",
    "    true_label = label_encoder.classes_[y_test[idx]]\n",
    "    \n",
    "    # Pr√©dire\n",
    "    prediction = model.predict(sample)\n",
    "    predicted_label = label_encoder.classes_[prediction[0]]\n",
    "    \n",
    "    # Probabilit√©s\n",
    "    probabilities = model.predict_proba(sample)[0]\n",
    "    \n",
    "    print(f\"\\nüìä √âchantillon #{idx}:\")\n",
    "    print(f\"   Features: {sample[0][:5]}... (premiers 5 valeurs)\")\n",
    "    print(f\"\\nüéØ Vraie classe: {true_label}\")\n",
    "    print(f\"ü§ñ Pr√©diction: {predicted_label}\")\n",
    "    print(f\"   {'‚úÖ CORRECT' if true_label == predicted_label else '‚ùå INCORRECT'}\")\n",
    "    \n",
    "    print(f\"\\nüìä Top 3 probabilit√©s:\")\n",
    "    top_3_idx = np.argsort(probabilities)[-3:][::-1]\n",
    "    for i, idx in enumerate(top_3_idx, 1):\n",
    "        label = label_encoder.classes_[idx]\n",
    "        prob = probabilities[idx]\n",
    "        print(f\"   {i}. {label:15s}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FONCTION PRINCIPALE\n",
    "# ============================================\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Pipeline complet d'entra√Ænement\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ü§ñ ENTRA√éNEMENT DU MOD√àLE DE RECONNAISSANCE ASL\")\n",
    "    print(\"=\"*70)\n",
    "   \n",
    "    # 1. Charger les donn√©es\n",
    "    df = load_and_explore_data()\n",
    "   \n",
    "    # 2. Pr√©traiter\n",
    "    X_train, X_test, y_train, y_test, scaler, label_encoder = preprocess_data(df)\n",
    "   \n",
    "    # 3. Entra√Æner\n",
    "    model = train_model(X_train, y_train)\n",
    "   \n",
    "    # 4. √âvaluer\n",
    "    accuracy, report, cm = evaluate_model(model, X_test, y_test, label_encoder)\n",
    "   \n",
    "    # AJOUT : Cr√©er le dossier de sortie AVANT la visualisation\n",
    "    os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"üìÅ Dossier de sortie cr√©√©: {MODEL_OUTPUT_DIR}\")\n",
    "   \n",
    "    # 5. Visualiser\n",
    "    feature_importance = model.feature_importances_\n",
    "    visualize_results(cm, label_encoder, feature_importance, MODEL_OUTPUT_DIR)\n",
    "   \n",
    "    # 6. Sauvegarder (le makedirs est redondant maintenant, mais inoffensif)\n",
    "    save_model(model, scaler, label_encoder, MODEL_OUTPUT_DIR)\n",
    "   \n",
    "    # 7. Test de pr√©diction\n",
    "    test_prediction(model, scaler, label_encoder, X_test, y_test)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ú® ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS !\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüìä R√©sum√©:\")\n",
    "    print(f\"   üéØ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"   üìÅ Mod√®le sauvegard√© dans: {MODEL_OUTPUT_DIR}\")\n",
    "    print(f\"   üè∑Ô∏è  Classes reconnues: {len(label_encoder.classes_)}\")\n",
    "    print(f\"   üìä Features utilis√©es: {len(FEATURE_COLUMNS)}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# EX√âCUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
